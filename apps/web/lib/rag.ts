/**
 * RAG (Retrieval Augmented Generation) system for documentation
 * Uses TF-IDF based search to find relevant documentation chunks
 *
 * Supports both:
 * 1. Pre-built index from build time (faster, generated by scripts/generate-rag-index.cjs)
 * 2. Runtime index building (fallback when pre-built index is not available)
 *
 * Also includes project knowledge chunks for self-awareness about:
 * - The Claude Insider project itself
 * - Author information (Vladimir Dukelic)
 * - Tech stack and architecture
 * - Voice assistant capabilities
 */

import fs from "fs";
import path from "path";
import matter from "gray-matter";
import { PROJECT_KNOWLEDGE_CHUNKS } from "../data/system-prompt";

// Document chunk for RAG
export interface DocumentChunk {
  id: string;
  title: string;
  section: string;
  content: string;
  url: string;
  category: string;
  keywords: string[];
  // Optional fields for specialized chunks
  subcategory?: string;
  isResource?: boolean;
  isConfigOption?: boolean;
  isCommand?: boolean;
  isEnvVar?: boolean;
  isFeature?: boolean;
  isExternalSource?: boolean;
  // Structured data for specific chunk types
  settingData?: {
    name: string;
    description: string;
    sourceDoc: string;
  };
  commandData?: {
    command: string;
    description: string;
    sourceDoc: string;
  };
  resourceData?: {
    id: string;
    category: string;
    tags: string[];
    featured: boolean;
  };
}

// Search result with relevance score
export interface SearchResult {
  chunk: DocumentChunk;
  score: number;
}

// Pre-built RAG index structure
interface RagIndex {
  version: string;
  generatedAt: string;
  documentCount: number;
  chunks: DocumentChunk[];
  tfidfIndex: Record<string, Record<string, number>>;
  idfValues: Record<string, number>;
}

// In-memory document index
let documentIndex: DocumentChunk[] | null = null;
let tfidfIndex: Map<string, Map<string, number>> | null = null;
let idfValues: Map<string, number> | null = null;
let indexLoadedFromPrebuilt = false;

/**
 * Tokenize text into words for search
 */
function tokenize(text: string): string[] {
  return text
    .toLowerCase()
    .replace(/[^\w\s]/g, " ")
    .split(/\s+/)
    .filter((word) => word.length > 2)
    .filter((word) => !STOP_WORDS.has(word));
}

// Common stop words to filter out
const STOP_WORDS = new Set([
  "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
  "of", "with", "by", "from", "as", "is", "was", "are", "were", "been",
  "be", "have", "has", "had", "do", "does", "did", "will", "would", "could",
  "should", "may", "might", "can", "this", "that", "these", "those", "it",
  "its", "you", "your", "we", "our", "they", "their", "he", "she", "his",
  "her", "what", "which", "who", "whom", "when", "where", "why", "how",
  "all", "each", "every", "both", "few", "more", "most", "other", "some",
  "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too",
  "very", "just", "also", "now", "here", "there", "then", "once", "if",
]);

/**
 * Try to load pre-built RAG index from build time
 */
function loadPrebuiltIndex(): boolean {
  try {
    const indexPath = path.join(process.cwd(), "data/rag-index.json");

    if (!fs.existsSync(indexPath)) {
      console.log("RAG: Pre-built index not found, will build at runtime");
      return false;
    }

    const indexContent = fs.readFileSync(indexPath, "utf-8");
    const ragIndex: RagIndex = JSON.parse(indexContent);

    // Load chunks
    documentIndex = ragIndex.chunks;

    // Convert tfidfIndex from object to Map
    tfidfIndex = new Map();
    for (const [docId, terms] of Object.entries(ragIndex.tfidfIndex)) {
      const termMap = new Map<string, number>();
      for (const [term, score] of Object.entries(terms)) {
        termMap.set(term, score);
      }
      tfidfIndex.set(docId, termMap);
    }

    // Convert idfValues from object to Map
    idfValues = new Map();
    for (const [term, score] of Object.entries(ragIndex.idfValues)) {
      idfValues.set(term, score);
    }

    indexLoadedFromPrebuilt = true;
    console.log(`RAG: Loaded pre-built index with ${ragIndex.documentCount} chunks (generated ${ragIndex.generatedAt})`);
    return true;
  } catch (error) {
    console.error("RAG: Error loading pre-built index:", error);
    return false;
  }
}

/**
 * Split content into chunks by sections (headers)
 */
function chunkContent(
  content: string,
  title: string,
  url: string,
  category: string
): DocumentChunk[] {
  const chunks: DocumentChunk[] = [];

  // Remove MDX components and code blocks for cleaner text
  const cleanContent = content
    .replace(/<[^>]+>/g, " ")
    .replace(/```[\s\S]*?```/g, "[code block]")
    .replace(/`[^`]+`/g, " ")
    .replace(/\{[^}]+\}/g, " ")
    .replace(/import\s+.*?from\s+['"][^'"]+['"]/g, "")
    .replace(/export\s+/g, "");

  // Split by headers (## or ###)
  const sections = cleanContent.split(/(?=^#{2,3}\s)/m);

  for (let i = 0; i < sections.length; i++) {
    const rawSection = sections[i];
    if (!rawSection) continue;
    const section = rawSection.trim();
    if (!section) continue;

    // Extract section title
    const headerMatch = section.match(/^#{2,3}\s+(.+?)$/m);
    const sectionTitle = headerMatch?.[1]?.trim() ?? title;

    // Get section content (without header)
    const sectionContent = headerMatch
      ? section.replace(/^#{2,3}\s+.+?\n/, "").trim()
      : section;

    if (sectionContent.length < 50) continue; // Skip very short sections

    // Extract keywords from content
    const words = tokenize(sectionContent);
    const wordFreq = new Map<string, number>();
    words.forEach((word) => {
      wordFreq.set(word, (wordFreq.get(word) || 0) + 1);
    });

    // Get top keywords by frequency
    const keywords = Array.from(wordFreq.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10)
      .map(([word]) => word);

    chunks.push({
      id: `${url}#${i}`,
      title,
      section: sectionTitle,
      content: sectionContent.slice(0, 1500), // Limit chunk size
      url,
      category,
      keywords,
    });
  }

  // If no sections found, use the whole content
  if (chunks.length === 0 && cleanContent.length > 50) {
    const words = tokenize(cleanContent);
    const wordFreq = new Map<string, number>();
    words.forEach((word) => {
      wordFreq.set(word, (wordFreq.get(word) || 0) + 1);
    });

    const keywords = Array.from(wordFreq.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10)
      .map(([word]) => word);

    chunks.push({
      id: `${url}#0`,
      title,
      section: title,
      content: cleanContent.slice(0, 1500),
      url,
      category,
      keywords,
    });
  }

  return chunks;
}

/**
 * Load and index all documentation files
 * First tries to load pre-built index, falls back to runtime building
 */
export function loadDocumentIndex(): DocumentChunk[] {
  // Return cached index if available
  if (documentIndex) return documentIndex;

  // Try to load pre-built index first
  if (loadPrebuiltIndex() && documentIndex) {
    return documentIndex;
  }

  // Fall back to runtime building
  console.log("RAG: Building index at runtime...");

  const contentDir = path.join(process.cwd(), "content");
  const chunks: DocumentChunk[] = [];

  // Category mapping
  const categories: Record<string, string> = {
    "getting-started": "Getting Started",
    configuration: "Configuration",
    "tips-and-tricks": "Tips & Tricks",
    api: "API Reference",
    integrations: "Integrations",
    tutorials: "Tutorials",
    examples: "Examples",
  };

  // Recursively find all MDX files
  function walkDir(dir: string, category: string = ""): void {
    if (!fs.existsSync(dir)) return;

    const files = fs.readdirSync(dir);

    for (const file of files) {
      const filePath = path.join(dir, file);
      const stat = fs.statSync(filePath);

      if (stat.isDirectory()) {
        const newCategory = category || file;
        walkDir(filePath, newCategory);
      } else if (file.endsWith(".mdx")) {
        try {
          const content = fs.readFileSync(filePath, "utf-8");
          const { data: frontmatter, content: mdxContent } = matter(content);

          const relativePath = path.relative(contentDir, filePath);
          const slug = relativePath
            .replace(/\.mdx$/, "")
            .replace(/\/index$/, "")
            .replace(/\\/g, "/");

          const url = `/docs/${slug}`;
          const title = frontmatter.title || slug.split("/").pop() || "Untitled";
          const categoryName = categories[category] || category;

          const fileChunks = chunkContent(mdxContent, title, url, categoryName);
          chunks.push(...fileChunks);
        } catch (error) {
          console.error(`Error processing ${filePath}:`, error);
        }
      }
    }
  }

  walkDir(contentDir);

  // Add project knowledge chunks for self-awareness
  chunks.push(...PROJECT_KNOWLEDGE_CHUNKS);

  documentIndex = chunks;

  // Build TF-IDF index
  buildTfidfIndex(chunks);

  console.log(`RAG: Built runtime index with ${chunks.length} chunks (including ${PROJECT_KNOWLEDGE_CHUNKS.length} project knowledge chunks)`);
  return chunks;
}

/**
 * Build TF-IDF index for search
 */
function buildTfidfIndex(chunks: DocumentChunk[]): void {
  tfidfIndex = new Map();
  const docFreq = new Map<string, number>();

  // Calculate term frequency for each document
  for (const chunk of chunks) {
    const words = tokenize(chunk.content + " " + chunk.title + " " + chunk.section);
    const termFreq = new Map<string, number>();

    for (const word of words) {
      termFreq.set(word, (termFreq.get(word) || 0) + 1);
    }

    // Normalize by document length
    const maxFreq = Math.max(...termFreq.values());
    for (const [term, freq] of termFreq) {
      termFreq.set(term, freq / maxFreq);
    }

    tfidfIndex.set(chunk.id, termFreq);

    // Track document frequency
    const uniqueTerms = new Set(words);
    for (const term of uniqueTerms) {
      docFreq.set(term, (docFreq.get(term) || 0) + 1);
    }
  }

  // Calculate IDF values
  idfValues = new Map();
  const numDocs = chunks.length;
  for (const [term, freq] of docFreq) {
    idfValues.set(term, Math.log(numDocs / (1 + freq)));
  }
}

/**
 * Detect query intent for better result ranking
 */
function detectQueryIntent(query: string): {
  isSettingQuery: boolean;
  isCommandQuery: boolean;
  isEnvVarQuery: boolean;
  isFeatureQuery: boolean;
  isResourceQuery: boolean;
} {
  const q = query.toLowerCase();
  return {
    isSettingQuery: /setting|config|option|preference|parameter|value|toggle|enable|disable/.test(q),
    isCommandQuery: /command|cli|how\s+to|run|execute|terminal|bash|shell|claude\s+\w+/.test(q),
    isEnvVarQuery: /env|environment|variable|api.?key|export/.test(q),
    isFeatureQuery: /feature|capability|can\s+(?:i|you|claude)|support|does/.test(q),
    isResourceQuery: /tool|library|mcp|server|sdk|resource|recommend/.test(q),
  };
}

/**
 * Search documents using TF-IDF scoring with context-aware boosting
 */
export function searchDocuments(
  query: string,
  limit: number = 5,
  context?: { type?: string; category?: string }
): SearchResult[] {
  const chunks = loadDocumentIndex();
  if (!tfidfIndex || !idfValues) return [];

  const queryTerms = tokenize(query);
  if (queryTerms.length === 0) return [];

  // Detect query intent for smart boosting
  const intent = detectQueryIntent(query);

  const scores: { chunk: DocumentChunk; score: number }[] = [];

  for (const chunk of chunks) {
    const docTf = tfidfIndex.get(chunk.id);
    if (!docTf) continue;

    let score = 0;

    // Calculate TF-IDF score
    for (const term of queryTerms) {
      const tf = docTf.get(term) || 0;
      const idf = idfValues.get(term) || 0;
      score += tf * idf;
    }

    // Boost for title/section matches
    const titleLower = chunk.title.toLowerCase();
    const sectionLower = chunk.section.toLowerCase();

    for (const term of queryTerms) {
      if (titleLower.includes(term)) score *= 1.5;
      if (sectionLower.includes(term)) score *= 1.3;
      if (chunk.keywords.includes(term)) score *= 1.2;
    }

    // Boost for exact phrase matches
    const queryLower = query.toLowerCase();
    if (chunk.content.toLowerCase().includes(queryLower)) {
      score *= 2;
    }

    // Context-aware boosting based on query intent
    if (intent.isSettingQuery && chunk.isConfigOption) {
      score *= 2.5; // Strong boost for settings when asking about configuration
    }
    if (intent.isCommandQuery && chunk.isCommand) {
      score *= 2.5; // Strong boost for commands when asking how to do something
    }
    if (intent.isEnvVarQuery && chunk.isEnvVar) {
      score *= 2.5; // Strong boost for env vars when asking about environment
    }
    if (intent.isFeatureQuery && chunk.isFeature) {
      score *= 2.0; // Boost features when asking about capabilities
    }
    if (intent.isResourceQuery && chunk.isResource) {
      score *= 2.0; // Boost resources when asking for recommendations
    }

    // Boost based on context if provided
    if (context?.category && chunk.category.toLowerCase().includes(context.category.toLowerCase())) {
      score *= 1.5;
    }
    if (context?.type === "code" && (chunk.isCommand || chunk.isConfigOption)) {
      score *= 1.3;
    }

    // Slight boost for structured data chunks (they have more precise info)
    if (chunk.settingData || chunk.commandData) {
      score *= 1.1;
    }

    if (score > 0) {
      scores.push({ chunk, score });
    }
  }

  // Sort by score and return top results
  return scores
    .sort((a, b) => b.score - a.score)
    .slice(0, limit);
}

/**
 * Get context for RAG from search results
 */
export function getRAGContext(
  query: string,
  maxChunks: number = 3,
  aiContext?: { type?: string; category?: string }
): string {
  const results = searchDocuments(query, maxChunks, aiContext);

  if (results.length === 0) {
    return "";
  }

  let context = "\n\nRELEVANT DOCUMENTATION:\n";

  for (const result of results) {
    const chunk = result.chunk;
    context += `\n---\n`;

    // Format based on chunk type for clearer context
    if (chunk.isConfigOption && chunk.settingData) {
      context += `[SETTING] ${chunk.settingData.name}\n`;
      context += `Description: ${chunk.settingData.description}\n`;
      context += `From: ${chunk.settingData.sourceDoc}\n`;
      context += `URL: ${chunk.url}\n`;
    } else if (chunk.isCommand && chunk.commandData) {
      context += `[COMMAND] ${chunk.commandData.command}\n`;
      if (chunk.commandData.description) {
        context += `Description: ${chunk.commandData.description}\n`;
      }
      context += `From: ${chunk.commandData.sourceDoc}\n`;
      context += `URL: ${chunk.url}\n`;
    } else if (chunk.isEnvVar) {
      context += `[ENV VARIABLE] ${chunk.title}\n`;
      context += chunk.content;
      context += `\nURL: ${chunk.url}\n`;
    } else if (chunk.isResource && chunk.resourceData) {
      context += `[RESOURCE] ${chunk.title}\n`;
      context += `Category: ${chunk.resourceData.category}\n`;
      if (chunk.resourceData.tags.length > 0) {
        context += `Tags: ${chunk.resourceData.tags.join(", ")}\n`;
      }
      context += chunk.content;
      context += `\nURL: ${chunk.url}\n`;
    } else {
      // Standard documentation chunk
      context += `[${chunk.category}] ${chunk.title}`;
      if (chunk.section !== chunk.title) {
        context += ` > ${chunk.section}`;
      }
      context += `\nURL: ${chunk.url}\n\n`;
      context += chunk.content;
    }
    context += `\n`;
  }

  return context;
}

/**
 * Get all available categories and their document counts
 */
export function getDocumentStats(): Record<string, number> {
  const chunks = loadDocumentIndex();
  const stats: Record<string, number> = {};

  for (const chunk of chunks) {
    stats[chunk.category] = (stats[chunk.category] || 0) + 1;
  }

  return stats;
}

/**
 * Check if the index was loaded from pre-built file
 */
export function isIndexPrebuilt(): boolean {
  return indexLoadedFromPrebuilt;
}

/**
 * Force rebuild of the index (useful for development)
 */
export function clearIndex(): void {
  documentIndex = null;
  tfidfIndex = null;
  idfValues = null;
  indexLoadedFromPrebuilt = false;
}
